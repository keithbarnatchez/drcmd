---
author: |
    | Keith Barnatchez
    | Harvard University
    | Department of Biostatistics
    | keithbarnatchez@g.harvard.edu
title: '`drcmd`: Doubly-Robust Causal Inference with Missing Data'
abstract: >
  
preamble: >
  \usepackage{amsmath}
# documentclass: jss
# classoption: article
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{`drcmd`: Doubly-Robust Causal Inference with Missing Data}
  %\VignetteEncoding{UTF-8}
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
</script>

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
source('../R/utils.R')
source('../R/drcmd.R')
source('../R/nuis.R')
source('../R/methods.R')
```

# Introduction \label{intro}

**Note**: Package is still in development. Vignette here will be very generic until package is ready for CRAN submission, but all necessary details on how to use the package will be outlined below.

# Technical Details

# Using `drcmd` \label{using}

## Installation

```{r install, eval=FALSE} 
devtools::install_github('keithbarnatchez/drcmd')
```

Note: Eventually we'll have a CRAN option as well

## Estimation of causal effects

To illustrate the use of the `drcmd` package, we will consider a missing data problem where the outcome of interest $Y$ is missing at random conditional on measured covariates $\mathbf X$: $Y \perp R | X$. We will assume a cheap, noisy proxy measurement for $Y$, denoted $Y^*$, is available for all subjects but not predictive of missingness (so that it is not necessary to satisfy the MAR assumption), resulting in the following simple data generating process:

$$
\begin{aligned}
X & \sim N(0,1) \ \ \ \ \ \  &\text{(Covariates)} \\
A|X & \sim \text{Bernoulli}(p = \text{logit}^{-1}(X)) \ \ \ \ \ \  &\text{(Treatment)} \\
Y|A,X & \sim N(\mu=1 + A + X, \sigma^2=1) \ \ \ \ \ \  &\text{(Outcome)} \\
Y^* & = Y + \varepsilon, \ \varepsilon \sim N(\mu=0,\sigma^2 = 1/4) \ \ \ \ \ \  &\text{(Outcome Proxy)} \\
\end{aligned}
$$

```{r simdata}
n <- 1e3
X <- rnorm(n) ; A <- rbinom(n,1,plogis(X)) ; Y <- rnorm(n) + A + X
Ystar <- Y + rnorm(n)/2 ; R <- rbinom(n,1,plogis(X)) ; X <- as.data.frame(X)
Y[R==0] <- NA # Make Y NA if R==0

df <- data.frame(Y=Y,A=A,X=X,Ystar=Ystar,R=R)
head(df)
```

The main function from the `drcmd` package is `drcmd()`. The core arguments are `Y`, `A` and `X`, representing the outcome, binary treatment and covariates. Users can optionally specify proxy variables `W` that are (i) predictive of the missing variables, (ii) possibly influence the missingness mechanism, and (iii) wouldn't be involved in a causal analysis under the presence of complete data. Such variables commonly arise in semi-supervised inference, where cheap proxies are often available for expensive-to-measure variables. In our running example, we have that $Y^*=W$. In practice, $W$ can be multi-dimensional when multiple proxies are available.  `W` defaults to `NULL` when not specified by the user, consistent with settings where proxies are not available.

Missing data are allowed in the outcome, treatment, and covariates (including any subset of covariates), as well as any combination of the three. The only requirement for running `drcmd` is that there exists at *least* one variable that is never missing, either in `Y`, `A`, `X`, `W`. Note that this does **not** guarantee identifiability of the causal estimands $\mathbb{E}[Y(1)]$ or $\mathbb{E}[Y(0)]$. The validity of the resulting estimates hinges on the MAR assumption holding, a crucial problem-specific determination that should be made through subject matter expertise.

Along with specifying variables, users can additionally specify means by which to estimate all nuisance functions. All nuisance functions can be estimated through either Super Learner (a stacking algorithm). There are 4 nuisance functions that are fit by `drcmd`, for $a = 0,1$:

$$
\begin{aligned}
&1. \  m_a(X) = \mathbb{E}(Y|A=a,X) \\
&2. \ g_a(X) = \mathbb{P}(A=a|X) \\
&3. \ r(Z)  = \mathbb{P}(R=1|Z) \\
&4. \ \varphi_a(Z) = \mathbb{E}(\chi_a(X,A,Y) | Z, R=1),
\end{aligned}
$$

where $\chi_a(X) = m_a(X) + \frac{I(A=a)}{g_a(X)}(Y-m_a(X))$ is the *pseudo-outcome* formed by the efficient influence function for estimating the counterfactul mean functional $\mathbb{E}[\mathbb{E}(Y|A=a,X)]$. Users can estimate nuisance functions through Super Learner via the `SuperLearner` package or the Highly-Adaptive LASSO (HAL) via the `hal9001` package. 

Users can specify learners for each nuisance function through nuisance-specific arguments `m_learners`, `g_learners`, `r_learners` and `po_learners`, as well as set default learners through the `default_learners` argument. To estimate a particular nuisance function through Super Learner, users can specify Super Learner libraries using the same syntax one passes directly into `SuperLearner`. To see the SuperLearner libraries available, users can run `get_sl_libraries()`:

```{r sl-libs}
get_sl_libraries()
```

Below we demonstrate an example call of `drcmd()` which specifies to estimate all nuisance functions with an ensemble of generalized linear models and splines.

```{r drcmd-simple}
res <- drcmd(Y=Y, A=A, X=X,
             default_learners=c('SL.glm','SL.earth'))
```

To make use of the additional proxy variables, we can simply specify the `W` argument in the call to `drcmd()`. In our running example, we have that $Y^*=W$.

```{r drcmd-proxy, eval=FALSE}
res <- drcmd(Y=Y, A=A, X=X, W=data.frame(Ystar),
             default_learners=c('SL.glm','SL.earth'))
```

Users can specify specific learners through nuisance-specific arguments, which will overwrite the learners specified in `default_learners` for that particular nuisance function if `default_learners` is specified. For example, to estimate the pseudo-outcome regression through generalized additive models (GAMs), and all other nuisance functions with a Super Learner ensemble of GLMs and splines, we can make the following call to `drcmd()`:

```{r drcmd-specific, eval=FALSE}
res <- drcmd::drcmd(Y=Y, A=A, X=X,
             default_learners=c('SL.glm','SL.earth'),
             po_learners = 'SL.gam')
```

Alternatively, one can omit specification of `default_learners` entirely, provided learners are specified for each nuisance function:

```{r drcmd-nuis-no-def, eval=FALSE}
res <- drcmd::drcmd(Y=Y, A=A, X=X,
             m_learners = c('SL.glm','SL.earth'),
             g_learners = 'SL.mean',
             r_learners = 'SL.glm.interaction',
             po_learners = c('SL.gam','SL.hal9001'))

```

Users can view a summary of the estimation procedure by calling the `summary()` function, which provides point estimates, standard errors and 95\% CIs for main causal estimands. By default, `drcmd` obtains estimates of $\mathbb{E}[Y(1)$, $\mathbb{E}[Y(0)]$, and the average treatment effect (ATE) $\mathbb{E}[Y(1)-Y(0)]$. 

```{r drcmd_summary}
summary(res)
```
 
### Extracting output

After running `drcmd()`, numerous objects are stored within the resulting output, including

- `results`: A list containing (i) parameter estimates stored in a dataframe named `estimates`, (ii) standard errors stored in a dataframe named `ses`, and (iii) nuisance function estimates stored in a dtaframe named `nuis`
- `params`: A list containing all parameter values used by `drcmd()`
- `R`: Binary complete case indicator, where 1 denotes a complete case
- `U`: Names of variables with partially missing values
- `Z`: Names of variables with no missing values

Users can obtain a detailed summary by specifying `detail=TRUE` in the `summary` function:

```{r drcmd_summary_detail}
summary(res,detail=TRUE)
```

## Additional features

### Diagnostic plots

While users can extract output from the results structure to construct plots manually, `drcmd` comes with numerous built-in plotting functions to help users diagnose potential issues in the fitting procedure.
Users can specify their desired plot with the `type` argument: (i) `PO`: residuals of psuedo-outcome regression vs predicted values, (ii) `IC`: density plots of the influence curves for $\mathbb{E}[Y(1)]$, $\mathbb{E}[Y(0)]$ and the ATE, (iii) `g_hat`: Density plots of fitted treatment propensity scores among complete cases, (iv) `r`_hat`: Density plots of fitted complete case propensity scores among complete cases.

```{r diagplotPO, fig.width=6, fig.height=3}
plot(res,type='PO')
```

Alternatively, users can cycle through all diagnostic plots by leaving the type argument unspecified or setting it to `'All'`

```{r diagplotPOAll, eval=FALSE}
plot(res)
```

### Cross-fitting

While not enabled by default, users can estimate parameters through cross-fitting by setting the `k` argument to the desired number of folds. By default, `drcmd` uses a single fold. Cross-fitting is encouraged when the users specifies nuisance learners that cover complex function classes, such as random forests. See the technical details section for more information on the rationale behind and implementation of cross-fitting.

```{r drcmd_simple, eval=FALSE}
res <- drcmd::drcmd(Y=Y, A=A, X=X,
             default_learners='SL.glm',
             k=3)
```

### Empirical efficiency maximixation

In practice, the pseudo-outcome regression function $\varphi_a(Z)$ will tend to be an inherently difficult nuisance function to estimate. While `drcmd` estimates this regression through conventional regression methods by default, users can optionally fit $\varphi_a$ through empirical efficiency maximization (EEM) by setting the argument `eem_ind` to `TRUE`. Given an implicitly-defined function class determined through choice of nuisance learner for $\varphi_a$, rather than attempt minimize the  MSE $\hat \varphi_a - \varphi_a||$, EEM aims to minimize the asymptotic variance of the causal estimand itself. An example function call is provided below:

```{r drcmd_eem, eval=FALSE}
res <- drcmd::drcmd(Y=Y, A=A, X=X,
             default_learners='SL.glm',
             k=1,
             eem_ind=TRUE)
```

### Bootstrap standard errors

By default, `drcmd` obtains standard errors through estimation of the asymptotic variance of the estimator. Particularly for small samples, users may wish to obtain standard errors through bootstrapping. Users can specify the number of bootstrap samples through the `nboot` argument. Below we demonstrate a call to `drcmd()` with 1000 bootstrap samples:

```{r drcmd_bootstrap, eval=FALSE}
res <- drcmd::drcmd(Y=Y, A=A, X=X,
             default_learners='SL.glm',
             k=1,
             nboot=1000)
```

# Technical Details


## Observed and full data influence functions

`drcmd` leverages developments from semiparametric theory for the estimation of functionals in the presence of missing data. Key to semiparametric efficient estimation with missing data is the conceptualization of (i) the *full-data* distribution one would have access to in the presence of missing data, and (ii) the *observed* data distribution. In full generality, suppose there exists missingness free distribution one would ideally sample from, denoted $\mathbb{P}_F$, containing i.i.d. observations
$$
F_i \sim \mathbb{P}_\text{F}
$$
where interest lies in some pathwise differentiable statistical functional $\Psi(\mathbb{P}_\text{F}$ and $F_i$ can be decomposed into $F_i = (V_i, U_i)$.

Rather than observe data from $\mathbb{P}_F$, we instead observe data from the *observed-data* distribution $\mathbb{P}_O$ containing i.i.d. observations
$$
O_i = (W_i, \ V_i, \ R_i U_i, \ R_i)\sim \mathbb{P}_O.
$$
Above, $U_i$ is only observed when $R_i=1$, and $W_i$ is observed for all $i$ and contains variables that are (i) possibly predictive of missingness, and (ii) but not a component of $F$. Letting $\chi(O, \mathbb{P}_\text{F})$ denote the efficient influence curve for $\Psi(\mathbb{P}_\text{F})$, the efficient influence curve for $\Psi(\mathbb{P}_\text{F})$ induced by the observed data distribution can be written
$$
\chi(O,\mathbb{P}_O) = \frac{R}{\mathbb{P}(R=1|Z)}\chi(F,\mathbb{P}_\text{F}) - \left(\frac{R}{\mathbb{P}(R=1|Z)} - 1 \right) \varphi(O) 
$$
where $\varphi(O) = \mathbb{E}[\chi(O,\mathbb{P}_\text{F}) | Z]$, where the above representation holds so long as the missing at random assumption $U \perp R | Z$ holds, where $Z = (W, V)$ collects all variables which are always observed.

To build intuition, return to the earlier outcome proxy example where we observe
$$
O_i = (R_i Y_i, A_i, X_i, Y_i^*) \sim \mathbb{P}_O
$$
In this setting, the idea distribution is given by $F = (Y, A, X)$, and notice $W=Y^*$, $Z = (Y^*, A, X)$ and $U = Y$. In turn,

### One-step estimation

### Targeted maximum likelihood estimation

Recalling the form of the influence curve,
$$
\frac{R}{\mathbb{P}(R=1|Z)}\chi(F,\mathbb{P}_\text{F}) - \left(\frac{R}{\mathbb{P}(R=1|Z)} - 1 \right) \varphi(O) 
$$
the targeted maximum likelihood approach aims

### Standard error estimation

For all estimands in `drcmd`, standard error estimation is based asymptotic variance formulae by default. Users can alternatively obtain standard errors through bootstrapping by setting the `nboot` argument to the desired number of bootstrap samples. 

**Counterfactual means**: Let $\hat \psi_a$ be an estimator of $\psi_a = \mathbb{E}[Y(a)]$, based on the observed data influence curve $\chi_a(O,\eta)$, where $\eta$ collects all nuisance functions and $\mathbb{E}[\chi_a(O,\eta)]=0$. It can be shown under modest regularity conditions on the estimation rates of all nuisance functions that
$$
\hat \psi_a - \psi_a = \frac{1}{n} \sum_{i=1}^n \chi_a(O_i, \eta) + o_p(n^{-1/2})
$$
In turn, the asymptotic variance of $\hat \psi_a$ is given by $\mathbb{E}[ \chi_a(O,\eta)^2]$. Standard error estimates are obtained by plugging in the empirical influence curve $\hat \chi_a(O,\hat \eta)$ for $\chi_a(O,\eta)$:
$$
\sqrt{\frac{1}{n^2} \sum_{i=1}^n \chi_a(O_i, \hat \eta)^2}
$$

**Risk ratio**: Let $\hat \psi_1$ and $\hat \psi_0$ be asymptotically linear estimators of $\psi_1$ and $\psi_0$, respectively, and let 
$$
\Sigma = \begin{pmatrix}
\sigma_1^2 & \nu \\
\nu & \sigma_0^2
\end{pmatrix}
$$
be the asymptotic covariance of $\hat \psi_1$ and $\hat \psi_0$. A straightforward application of the multivariate delta method implies the asymptotic variance of the risk ratio estimator $\hat \psi_1/\hat \psi_0$ is given by  
$$
\begin{aligned}
\nabla g^\top \Sigma \nabla  g
\end{aligned}
$$
where $\nabla g = (1/\psi_1, -\psi_1/\psi_0^2)$ is the gradient of $g(\psi_1,\psi_0) = \psi_1/\psi_0$. Evaluating the above expression yields
$$
\begin{aligned}
\frac{\sigma_1^2}{\psi_0^2} + \frac{\sigma_0^2 \psi_1^2}{\psi_0^4} - \frac{2\nu \psi_1}{\psi_0^3}.
\end{aligned}
$$
`drcmd` estimates the above asymptotic variance by substituting estimates for each component. 

**Odds ratio**: Continue to let $\hat \psi_1$ and $\hat \psi_0$ be asymptotically linear estimators of $\psi_1$ and $\psi_0$, respectively. One can (i) find the asymptotic variance of the *log* odds ratio estimator $\log(\hat \psi_1/\hat \psi_0)$, and (ii) apply the delta method an additional time to obtain the asymptotic variance of the odds ratio estimator $\hat \psi_\text{OR} = \hat \psi_1/(1-\hat\psi_1) \big/\hat \psi_0 /(1-\hat \psi_0)$:
$$
\begin{aligned}
\psi_\text{OR}^2 \left( \frac{\sigma_1^2}{\psi_1^2(1-\psi_1)^2} + \frac{\sigma_0^2}{\psi_0^2(1-\psi_0)^2} - \frac{2\nu}{\psi_1(1-\psi_1)\psi_0(1-\psi_0)} \right).
\end{aligned}
$$


### Cross-fitting estimation

 






